{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f400486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae39ff",
   "metadata": {},
   "source": [
    "# Object masks in images from prompts with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b25c",
   "metadata": {},
   "source": [
    "Segment Anything Model 2 (SAM 2) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt. \n",
    "\n",
    "The `SAM2ImagePredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee822903-7739-4c1b-941a-b292b6e89bcf",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644532a8",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fabfee",
   "metadata": {},
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16b43f9-8727-4aab-9656-2d44c6d1b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be845da",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33681dd1",
   "metadata": {},
   "source": [
    "Necessary imports and helper functions for displaying points, boxes, and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b28288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a15e2f-c7e1-4e5d-862f-fcb751a60b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bc90d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def show_mask_all(image, mask, box_coords=None, input_labels=None, borders=False):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca(), borders=borders)\n",
    "    if box_coords is not None:\n",
    "        # boxes\n",
    "        show_box(box_coords, plt.gca())\n",
    "    plt.axis('off')\n",
    "    if input_labels is not None:\n",
    "        plt.savefig(input_labels, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b228b8",
   "metadata": {},
   "source": [
    "## Selecting objects with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1927b",
   "metadata": {},
   "source": [
    "First, load the SAM 2 model and predictor. Change the path below to point to the SAM 2 checkpoint. Running on CUDA and using the default model are recommended for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e28150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /home/yangmi/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "!sed -i 's/from torchvision.transforms.functional_tensor import rgb_to_grayscale/from torchvision.transforms.functional import rgb_to_grayscale/' /home/yangmi/.conda/envs/SAM2/lib/python3.11/site-packages/basicsr/data/degradations.py\n",
    "\n",
    "import os, sys\n",
    "import gc\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from utils.supervision_utils import CUSTOM_COLOR_MAP\n",
    "from grounding_dino.groundingdino.models import build_model\n",
    "from grounding_dino.groundingdino.util.inference import load_model, load_image, predict\n",
    "from grounding_dino.groundingdino.util.slconfig import SLConfig\n",
    "from grounding_dino.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "## annotation\n",
    "from annotator.hed import HEDdetector, nms\n",
    "from annotator.util import HWC3, resize_image\n",
    "\n",
    "import supervision as sv\n",
    "from scipy import ndimage\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import traceback\n",
    "import shutil\n",
    "\n",
    "\n",
    "def load_model_hf(repo_id, filename, ckpt_config_filename):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file)\n",
    "    model = build_model(args)\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    #checkpoint = torch.load(cache_file, map_location=device)\n",
    "    checkpoint = torch.load(cache_file)\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    #model.to(device)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def instance_outline_extraction_by_mask(grounding_model, sam2_predictor, input_dir, output_dir, img_format = 'png', image_resolution = 1024, expand_pixel=3, device='cuda'):\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_filename_list = [i for i in os.listdir(input_dir)]\n",
    "    images_path = [os.path.join(input_dir, file_path)\n",
    "                        for file_path in image_filename_list]\n",
    "\n",
    "    hedDetector = HEDdetector()\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    image_dim = 1024\n",
    "\n",
    "    instance_types = {'product': ['beauty product', 'cosmetic product', 'skincare product', 'makeup product', 'personal care product', 'gift boxes'],\n",
    "                      'human': ['human faces', 'faces', 'human hands', 'hands'],\n",
    "                      'botanic': ['flowers', 'blossom', 'plants', 'bush'],\n",
    "                      'landmark': ['rocks', 'tables', 'stairs', 'windows', 'mirror'],\n",
    "                      'effect': ['shadows', 'water splash', 'bubbles', 'lighting']\n",
    "    }\n",
    "\n",
    "    \n",
    "    for img_path, img_name in zip(images_path, image_filename_list):\n",
    "        #####################################\n",
    "        img_id = '.'.join(img_name.split('.')[:-1])\n",
    "        #extract mask\n",
    "        image_source, image = load_image(img_path, image_dim)\n",
    "\n",
    "        for type_name in instance_types.keys():\n",
    "            for obj_name in instance_types[type_name]:\n",
    "                det_boxes, _, phrases = predict(\n",
    "                        model=grounding_model,\n",
    "                        image=image,\n",
    "                        caption=obj_name,\n",
    "                        box_threshold=0.35,\n",
    "                        text_threshold=0.25\n",
    "                    )\n",
    "        \n",
    "                sam2_predictor.set_image(image_source)\n",
    "                mask_all = np.full((image_source.shape[1],image_source.shape[1]), True, dtype=bool)\n",
    "                        \n",
    "                if det_boxes.size(0) != 0:\n",
    "                    masks, scores, logits = sam2_predictor.predict(\n",
    "                        point_coords=None,\n",
    "                        point_labels=None,\n",
    "                        box=det_boxes,\n",
    "                        multimask_output=False,\n",
    "                    )\n",
    "                    \"\"\"\n",
    "                    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "                    \"\"\"\n",
    "                    # convert the shape to (n, H, W)\n",
    "                    if masks.ndim == 4:\n",
    "                        masks = masks.squeeze(1)\n",
    "\n",
    "                    sorted_ind = np.argsort(scores)[::-1]\n",
    "                    masks = masks[sorted_ind]\n",
    "                    scores = scores[sorted_ind]\n",
    "                    logits = logits[sorted_ind]\n",
    "\n",
    "                    for mask in masks:\n",
    "                        im = np.stack((mask,)*3, axis=-1)\n",
    "                        im = im.astype(np.uint8)*255\n",
    "                        imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "                        _, thresh = cv2.threshold(imgray, 127, 255, 0)\n",
    "                        contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                        if len(contours) >= 50:\n",
    "                            continue\n",
    "                        mask_all = mask_all & ~mask.astype(bool)\n",
    "                \n",
    "                    input_labels = output_dir + '/' + img_id + '_' + obj_name + '_' + str(input_ind) + '.' + img_format\n",
    "                    show_mask_all(image, mask_all, box_coords=None, input_labels=None, borders=False)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"the outline of {obj_name} in {img_name} cannot be extracted.\")\n",
    "                    #raise ValueError(f\"the outline of {obj_name} in {img_name} cannot be extracted.\")\n",
    "            \n",
    "            if len(np.unique(mask_all)) != 2:\n",
    "                print(f\"No mask for this instance >~< {img_id}.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "# build dino\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename).to(device)\n",
    "\n",
    "# build sam-2\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5adf09-c830-4b95-a98e-fbe8c15fc287",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# copy path to make files writable\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy2(\u001b[43mimg_path\u001b[49m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, img_name))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# run inference\u001b[39;00m\n\u001b[1;32m     70\u001b[0m instance_outline_extraction_by_mask(groundingdino_model, sam2_predictor, \n\u001b[1;32m     71\u001b[0m                                     input_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39minput_dir, output_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[1;32m     72\u001b[0m                                     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     73\u001b[0m                                    )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_path' is not defined"
     ]
    }
   ],
   "source": [
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of product position extraction based on Grouned SAM.\")\n",
    "\n",
    "    parser.add_argument(\"--input_dir\", \n",
    "                        default=None, \n",
    "                        type=str, \n",
    "                        required=True, \n",
    "                        help=\"Path to data instance.\")\n",
    "    \n",
    "    parser.add_argument(\"--data_hed_dir\", \n",
    "                        default=None, \n",
    "                        type=str, \n",
    "                        required=False, \n",
    "                        help=\"Path to data hed.\")\n",
    "\n",
    "    parser.add_argument(\"--output_dir\", \n",
    "                        default=None, \n",
    "                        type=str, \n",
    "                        required=True, \n",
    "                        help=\"Path to data hed background.\")\n",
    "    \n",
    "    parser.add_argument(\"--img_format\", \n",
    "                        default='png', \n",
    "                        type=str, \n",
    "                        help=\"Path to the image.\")\n",
    "    \n",
    "    parser.add_argument(\"--gpu_id\", \n",
    "                        default=0, \n",
    "                        type=int, \n",
    "                        required=False,\n",
    "                        help=\"gpu id\")\n",
    "\n",
    "    parser.add_argument(\"--product_images\",\n",
    "                    nargs='+', \n",
    "                    default=None,\n",
    "                    required=False,\n",
    "                    help=\"The background image with the product\")\n",
    "\n",
    "    parser.add_argument(\"--similarity_threshold\", \n",
    "                        default=2.5,#0.916, \n",
    "                        type=float, \n",
    "                        required=False,\n",
    "                        help=\"The threshold to remove hed images\")\n",
    "\n",
    "    parser.add_argument(\"--hed_value\", \n",
    "                        default=190, \n",
    "                        type=int, \n",
    "                        required=False,\n",
    "                        help=\"The hed value for product\")\n",
    "\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_args(input_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "args = parse_args(['--input_dir', '/mys3bucket/beauty-lvm/v1/controlnola-controlnet-trainingdata/batch1', '--output_dir', '../tmp_seg'])\n",
    "device = torch.device(args.gpu_id)\n",
    "    \n",
    "# use float16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda:\"+str(args.gpu_id), dtype=torch.float16).__enter__()\n",
    "torch.autocast(device_type=\"cuda:0\", dtype=torch.float16).__enter__()\n",
    "\n",
    "# copy path to make files writable\n",
    "\n",
    "shutil.copy2(img_path, os.path.join(output_dir, img_name))\n",
    "\n",
    "# run inference\n",
    "instance_outline_extraction_by_mask(groundingdino_model, sam2_predictor, \n",
    "                                    input_dir=args.input_dir, output_dir=args.output_dir,\n",
    "                                    device=device,\n",
    "                                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925e829",
   "metadata": {},
   "source": [
    "Process the image to produce an image embedding by calling `SAM2ImagePredictor.set_image`. `SAM2ImagePredictor` remembers this embedding and will use it for subsequent mask prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc7a46",
   "metadata": {},
   "source": [
    "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7f6fe-f772-4008-bb85-61ef7dc46bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
